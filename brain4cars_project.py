# -*- coding: utf-8 -*-
"""Brain4cars_project _dorostshode35.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XlxlcsTTPRzfDf4nctZDr5yWogH6K9Ww
"""



# -*- coding: utf-8 -*-
"""inception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qAf1msCeE23CExPaClzoofiZ5ik8Tc--
"""

# -*- coding: utf-8 -*-
"""resnet3d_virayesh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fp6Mil7vbQ1f3raDEanjnuKe4icsq8JL
"""

##library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pnd
from tqdm import tqdm
import sys
import math
import os
import zipfile
import six
import warnings
import random
import gc
import six
from math import ceil
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import cv2
import imgaug as ia
from imgaug import augmenters as iaa
from moviepy.editor import VideoFileClip
from PIL import Image

from tensorflow.keras.utils import to_categorical

from scipy.io import loadmat
import keras.backend as K
#from keras.engine.topology import get_source_inputs
from keras.layers import Activation
from keras.layers import AveragePooling3D
from keras.layers import BatchNormalization
from keras.layers import Conv3D
from keras.layers import Conv3DTranspose
from keras.layers import Dense
from keras.layers import Dropout,Flatten
from keras.layers import GlobalAveragePooling3D
from keras.layers import GlobalMaxPooling3D
from keras.layers import Input
from keras.layers import MaxPooling3D
from keras.layers import Reshape
from keras.layers import UpSampling3D
from keras.layers import concatenate
from tensorflow.keras.models import Model
from keras.regularizers import l2
!pip install git+https://www.github.com/keras-team/keras-contrib.git
from keras_contrib.layers import SubPixelUpscaling
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import SGD,Adam

##download dataset
url = "https://ucfdce4cb589df90826e2f6047aa.dl.dropboxusercontent.com/zip_download_get/A-ntDQ-Jk7QAyIy3vTef0EU3bjOb2Z8Vcajy6N4Avw2Ae0AOgdY8i85lCY71CuYRG8rHvMrE2j8_IgfpSiJjtV5O7ovEe1yhILSze9Rne29VLA?_download_id=222127156016236786534297024169151449702373017361059476950349141253&_notify_domain=www.dropbox.com&dl=1"







target_path = '/content/brain.zip'
import requests, zipfile, io
response = requests.get(url, stream=True)
handle = open(target_path, "wb")
for chunk in response.iter_content(chunk_size=100):
     if chunk:  # filter out keep-alive new chunks
        handle.write(chunk)
handle.close()

##extract data
import zipfile
zip_ref = zipfile.ZipFile('/content/brain.zip', 'r')
zip_ref.extractall('/content/home/train')
zip_ref.close()

##prepare data
road = '/content/home/train/brain4cars_data/road_camera/'
face = '/content/home/train/brain4cars_data/face_camera/'
face_dir = os.listdir("/content/home/train/brain4cars_data/face_camera/")
face_dir = np.sort(face_dir)
face_filename=[]
face_label=[]
road_filename=[]
for i in range(0,5):
  path = face + face_dir[i]
  path_road = road + face_dir[i]
  action = os.listdir(path)
  action = np.sort(action)
  road_chek = os.listdir(path_road)
  for j in range(len(action)):
    if action[j]+'.avi' in road_chek:

      
      video_path = path+'/'+action[j]+'/video_'+action[j]+'.avi'
      video_path1 = path_road+'/'+action[j]+'.avi'
      
      try:
        clip = VideoFileClip(video_path)
        clip1 = VideoFileClip(video_path1)
        a = int(clip.duration)
        b = int(clip1.duration)
      except:
        a = 1
        b = 2

      
      
      if a==b:
        face_filename.append(video_path)
        face_label.append(i)
        road_filename.append(video_path1)
        gc.collect()



import imgaug as ia
from imgaug import augmenters as iaa
import numpy as np


sometimes = lambda aug: iaa.Sometimes(0.5, aug)

aug = iaa.Sequential(
    [
        iaa.SomeOf((0, 5),
            [
                iaa.OneOf([
                    iaa.GaussianBlur((0,0.5)), 
                    iaa.AverageBlur(k=(1, 3)), 
                    iaa.MedianBlur(k=(1, 3)),
                ]),

                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.03*255), per_channel=0.5),
                    iaa.Dropout((0.01, 0.03), per_channel=0.5),
                  
                ]),
                 
                
                iaa.OneOf([
                    iaa.Multiply((1, 1.25), per_channel=0.5),
                    iaa.Add((-25, 0), per_channel=0.5)

                ]),
                iaa.ContrastNormalization((1, 1.2 ), per_channel=0.5), 
            ],
            random_order=True
        )



##data generator
from imgaug import augmenters as iaa
import numpy as np
import keras
import tensorflow as tf
#from keras.utils import to_categorical
keras = tf.compat.v1.keras
Sequence = tf.keras.utils.Sequence
class DataGenerator(Sequence):
    def __init__(self, list_IDs, labels,save_path, batch_size=32,
                 n_classes=5, shuffle=True):
        
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.save_path=save_path
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        
        # Generate data
        X, y= self.__data_generation(list_IDs_temp)
        X=np.array(X)
        y=np.array(y)
        return X, y
    def length_of_video(self,length):
        cap=cv2.VideoCapture(video_path)
        length =int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        return length

        

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp,sample_rate=5):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X = []
        y = []
        X1=[]
        save_path='/content/home'
        
        # Generate data and augment
        for i, ID in enumerate(list_IDs_temp):
            cap=cv2.VideoCapture(ID)
            length =int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            frames=[]
            count=0
            while cap.isOpened():
                ret,frame=cap.read()
                
                
                if  ret==True and count%sample_rate ==0:
                          
                          frame=cv2.resize(frame, (128,128))
                          #frames=list(frames)
                          frame=aug.augment_image(frame)
                          frames.append(frame)


                elif ret==False or len(frames)==30:
                          break
                count=+1
                
            
            #frames=np.array(frames)
            # Store sample
            #X.append(frames)
            frames = np.array(frames)
            #print("sh",frames.shape)
            #X=np.reshape(X,(1,128,128,3))
            #frames=aug.augment_images(frames)
            #print(frames.shape)
            X.append(frames)
            #print(len(X))
            
            # Store class
            y.append(self.labels[i])
        #frames=aug.augment_images(frames)
        #X.append(frames)
        X=np.array(X)
        #X1 = np.array(X1)
        #X=np.reshape(X,(32,7168,224,3))
        X=np.reshape(X,(32,128,128,3))
        #X=np.resize(X,(32,150,150,3))

        
        #X=aug.augment_images(X)


        return X,tf.keras.utils.to_categorical(y, num_classes=self.n_classes)

datagen=DataGenerator(list_IDs=face_filename, labels=face_label,save_path='/content/home', batch_size=32,n_classes=5, shuffle=True)
X,y=datagen.__getitem__(index=1)
X.shape

##good
import tensorflow as tf
from tensorflow.keras import regularizers
#from keras import applications
#from tensorflow.keras.applications.Inception_V3 import InceptionV3
base_model = tf.keras.applications.InceptionResNetV2(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')

x = Flatten()(base_model.output)
x = Dense(48, kernel_regularizer=regularizers.l1_l2(l1=1e-1, l2=1e-1), activation='relu')(x)
x = BatchNormalization()(x)
#x = Dropout(0.6)(x)
x = Dense(5, bias_regularizer=l2(0.01),activation='sigmoid')(x)

model = tf.keras.models.Model(base_model.input, x)

model.compile(optimizer = SGD(lr=0.001), loss = 'categorical_crossentropy', metrics = ['acc'])

import tensorflow as tf
#from keras import applications
#from tensorflow.keras.applications.Inception_V3 import InceptionV3
#base_model = tf.keras.applications.inception_v3.InceptionV3(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')
base_model= tf.keras.applications.EfficientNetB0(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet')


x = Flatten()(base_model.output)
x = Dense(48, kernel_regularizer=regularizers.l1_l2(l1=1e-1, l2=1e-1), activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(5, activation='sigmoid')(x)

model = tf.keras.models.Model(base_model.input, x)

model.compile(optimizer = SGD(lr=0.01), loss = 'categorical_crossentropy', metrics = ['acc'])

base_model.output



# Fit data to model
  ##split data
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(face_filename, face_label, test_size=0.33, random_state=42)
  train_datagen=DataGenerator(list_IDs=X_train, labels=y_train,save_path='/content/home', batch_size=32,n_classes=5, shuffle=False)
  test_datagen=DataGenerator(list_IDs=X_test, labels=y_test,save_path='/content/home', batch_size=32,n_classes=5, shuffle=False)
  

  history=model.fit_generator(
    train_datagen,
    steps_per_epoch=np.ceil((len(X_train)/32)-1),
    epochs=50,
    validation_data=test_datagen,
    validation_steps=np.ceil((len(X_test)/32)-1))

pip install keras==2.7.0